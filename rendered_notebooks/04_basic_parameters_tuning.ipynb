{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to scikit-learn: basic model hyper-parameters tuning\n",
    "\n",
    "The process to learn a predictive model is driven by a set of internal\n",
    "parameters and a set of training data. These internal parameters are called\n",
    "hyper-parameters and are specific for each family of models. In addition,\n",
    "a set of parameters are optimal for a specific dataset and thus they need\n",
    "to be optimized.\n",
    "\n",
    "This notebook shows:\n",
    "* the influence of changing model parameters;\n",
    "* how to tune these hyper-parameters;\n",
    "* how to evaluate the model performance together with hyper-parameters\n",
    "  tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://www.openml.org/data/get_csv/1595261/adult-census.csv\")\n",
    "# Or use the local copy:\n",
    "# df = pd.read_csv('../datasets/adult-census.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' >50K', ..., ' <=50K', ' <=50K', ' >50K'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_name = \"class\"\n",
    "target = df[target_name].to_numpy()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   workclass      education  education-num       marital-status  \\\n",
       "0   25     Private           11th              7        Never-married   \n",
       "1   38     Private        HS-grad              9   Married-civ-spouse   \n",
       "2   28   Local-gov     Assoc-acdm             12   Married-civ-spouse   \n",
       "3   44     Private   Some-college             10   Married-civ-spouse   \n",
       "4   18           ?   Some-college             10        Never-married   \n",
       "\n",
       "           occupation relationship    race      sex  capital-gain  \\\n",
       "0   Machine-op-inspct    Own-child   Black     Male             0   \n",
       "1     Farming-fishing      Husband   White     Male             0   \n",
       "2     Protective-serv      Husband   White     Male             0   \n",
       "3   Machine-op-inspct      Husband   Black     Male          7688   \n",
       "4                   ?    Own-child   White   Female             0   \n",
       "\n",
       "   capital-loss  hours-per-week  native-country  \n",
       "0             0              40   United-States  \n",
       "1             0              50   United-States  \n",
       "2             0              40   United-States  \n",
       "3             0              40   United-States  \n",
       "4             0              30   United-States  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.drop(columns=[target_name, \"fnlwgt\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is loaded, we split it into a training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the preprocessing pipeline to transform differently\n",
    "the numerical and categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "binary_encoding_columns = ['sex']\n",
    "one_hot_encoding_columns = [\n",
    "    'workclass', 'education', 'marital-status', 'occupation',\n",
    "    'relationship', 'race', 'native-country']\n",
    "scaling_columns = [\n",
    "    'age', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "    'education-num']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('binary-encoder', OrdinalEncoder(), binary_encoding_columns),\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown='ignore'),\n",
    "     one_hot_encoding_columns),\n",
    "    ('standard-scaler', StandardScaler(), scaling_columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use a linear classifier (i.e. logistic regression) to predict\n",
    "whether or not a person earn more than 50,000 dollars a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score using a Pipeline is 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor, LogisticRegression(max_iter=1000, solver='lbfgs'))\n",
    "model.fit(df_train, target_train)\n",
    "print(f\"The accuracy score using a {model.__class__.__name__} is \"\n",
    "      f\"{model.score(df_test, target_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The issue of finding the best model parameters\n",
    "\n",
    "In the previous example, we created a `LogisticRegression` classifier using\n",
    "the default parameters by omitting setting explicitly these parameters.\n",
    "\n",
    "For this classifier, the parameter `C` governes the penalty; in other\n",
    "words, how much our model should \"trust\" (or fit) the training data.\n",
    "\n",
    "Therefore, the default value of `C` is never certified to give the best\n",
    "performing model.\n",
    "\n",
    "We can make a quick experiment by changing the value of `C` and see the\n",
    "impact of this parameter on the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score using a Pipeline is 0.86 with C=1\n"
     ]
    }
   ],
   "source": [
    "C = 1\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression(C=C, max_iter=1000, solver='lbfgs'))\n",
    "model.fit(df_train, target_train)\n",
    "print(f\"The accuracy score using a {model.__class__.__name__} is \"\n",
    "      f\"{model.score(df_test, target_test):.2f} with C={C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score using a Pipeline is 0.77 with C=1e-05\n"
     ]
    }
   ],
   "source": [
    "C = 1e-5\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression(C=C, max_iter=1000, solver='lbfgs'))\n",
    "model.fit(df_train, target_train)\n",
    "print(f\"The accuracy score using a {model.__class__.__name__} is \"\n",
    "      f\"{model.score(df_test, target_test):.2f} with C={C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best model hyper-parameters via exhaustive parameters search\n",
    "\n",
    "We see that the parameter `C` as a significative impact on the model\n",
    "performance. This parameter should be tuned to get the best cross-validation\n",
    "score, so as to avoid over-fitting problems.\n",
    "\n",
    "In short, we will set the parameter, train our model on some data, and\n",
    "evaluate the model performance on some left out data. Ideally, we will select\n",
    "the parameter leading to the optimal performance on the testing set.\n",
    "Scikit-learn provides a `GridSearchCV` estimator which will handle the\n",
    "cross-validation and hyper-parameter search for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor, LogisticRegression(max_iter=1000, solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see that we need to provide the name of the parameter to be set.\n",
    "Thus, we can use the method `get_params()` to have the list of the parameters\n",
    "of the model which can set during the grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hyper-parameters are for a logistic regression model are:\n",
      "C\n",
      "class_weight\n",
      "dual\n",
      "fit_intercept\n",
      "intercept_scaling\n",
      "l1_ratio\n",
      "max_iter\n",
      "multi_class\n",
      "n_jobs\n",
      "penalty\n",
      "random_state\n",
      "solver\n",
      "tol\n",
      "verbose\n",
      "warm_start\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"The hyper-parameters are for a logistic regression model are:\")\n",
    "for param_name in LogisticRegression().get_params().keys():\n",
    "    print(param_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hyper-parameters are for the full-pipeline are:\n",
      "memory\n",
      "steps\n",
      "verbose\n",
      "columntransformer\n",
      "logisticregression\n",
      "columntransformer__n_jobs\n",
      "columntransformer__remainder\n",
      "columntransformer__sparse_threshold\n",
      "columntransformer__transformer_weights\n",
      "columntransformer__transformers\n",
      "columntransformer__verbose\n",
      "columntransformer__binary-encoder\n",
      "columntransformer__one-hot-encoder\n",
      "columntransformer__standard-scaler\n",
      "columntransformer__binary-encoder__categories\n",
      "columntransformer__binary-encoder__dtype\n",
      "columntransformer__one-hot-encoder__categorical_features\n",
      "columntransformer__one-hot-encoder__categories\n",
      "columntransformer__one-hot-encoder__drop\n",
      "columntransformer__one-hot-encoder__dtype\n",
      "columntransformer__one-hot-encoder__handle_unknown\n",
      "columntransformer__one-hot-encoder__n_values\n",
      "columntransformer__one-hot-encoder__sparse\n",
      "columntransformer__standard-scaler__copy\n",
      "columntransformer__standard-scaler__with_mean\n",
      "columntransformer__standard-scaler__with_std\n",
      "logisticregression__C\n",
      "logisticregression__class_weight\n",
      "logisticregression__dual\n",
      "logisticregression__fit_intercept\n",
      "logisticregression__intercept_scaling\n",
      "logisticregression__l1_ratio\n",
      "logisticregression__max_iter\n",
      "logisticregression__multi_class\n",
      "logisticregression__n_jobs\n",
      "logisticregression__penalty\n",
      "logisticregression__random_state\n",
      "logisticregression__solver\n",
      "logisticregression__tol\n",
      "logisticregression__verbose\n",
      "logisticregression__warm_start\n"
     ]
    }
   ],
   "source": [
    "print(\"The hyper-parameters are for the full-pipeline are:\")\n",
    "for param_name in model.get_params().keys():\n",
    "    print(param_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `'logisticregression__C'` is the parameter for which we would\n",
    "like different values. Let see how to use the `GridSearchCV` estimator for\n",
    "doing such search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score using a GridSearchCV is 0.86 in 12.139 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "param_grid = {'logisticregression__C': (0.1, 1.0, 10.0)}\n",
    "model_grid_search = GridSearchCV(model, param_grid=param_grid,\n",
    "                                 n_jobs=4, cv=5)\n",
    "start = time.time()\n",
    "model_grid_search.fit(df_train, target_train)\n",
    "elapsed_time = time.time() - start\n",
    "print(\n",
    "    f\"The accuracy score using a {model_grid_search.__class__.__name__} is \"\n",
    "    f\"{model_grid_search.score(df_test, target_test):.2f} in \"\n",
    "    f\"{elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GridSearchCV` estimator takes a `param_grid` parameter which defines\n",
    "all possible parameters combination. Once the grid-search fitted, it can be\n",
    "used as any other predictor by calling `predict` and `predict_proba`.\n",
    "Internally, it will use the model with the best parameters found during\n",
    "`fit`. You can know about these parameters by looking at the `best_params_`\n",
    "attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best set of parameters is: {'logisticregression__C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"The best set of parameters is: {model_grid_search.best_params_}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `GridSearchCV` estimator, the parameters need to be specified\n",
    "explicitely. Instead, one could randomly generate (following a specific\n",
    "distribution) the parameter candidates. The `RandomSearchCV` allows for such\n",
    "stochastic search. It is used similarly to the `GridSearchCV` but the\n",
    "sampling distributions need to be specified instead of the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score using a RandomizedSearchCV is 0.86\n",
      "The best set of parameters is: {'logisticregression__C': 106.63992846066772}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    'logisticregression__C': uniform(loc=50, scale=100)}\n",
    "model_grid_search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_distributions, n_iter=3,\n",
    "    n_jobs=4, cv=5)\n",
    "model_grid_search.fit(df_train, target_train)\n",
    "print(\n",
    "    f\"The accuracy score using a {model_grid_search.__class__.__name__} is \"\n",
    "    f\"{model_grid_search.score(df_test, target_test):.2f}\")\n",
    "print(\n",
    "    f\"The best set of parameters is: {model_grid_search.best_params_}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on search efficiency\n",
    "\n",
    "Be aware that sometimes, scikit-learn provides some `EstimatorCV` classes\n",
    "which will perform internally the cross-validation in such way that it will\n",
    "more computationally efficient. We can give the example of the\n",
    "`LogisticRegressionCV` which can be used to find the best `C` in a more\n",
    "efficient way than what we previously did with the `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to train LogisticRegressionCV: 5.696 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# define the different Cs to try out\n",
    "param_grid = {\"C\": (0.1, 1.0, 10.0)}\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegressionCV(Cs=param_grid['C'], max_iter=1000,\n",
    "                         solver='lbfgs', n_jobs=4, cv=5))\n",
    "start = time.time()\n",
    "model.fit(df_train, target_train)\n",
    "elapsed_time = time.time() - start\n",
    "print(f\"Time elapsed to train LogisticRegressionCV: \"\n",
    "      f\"{elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` time for the `CV` version of `LogisticRegression` give a speed-up\n",
    "x2. This speed-up is provided by re-using the values of coefficients to\n",
    "warm-start the estimator for the different `C` values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises:\n",
    "\n",
    "- Build a machine learning pipeline:\n",
    "      * preprocess the categorical columns using an `OrdinalEncoder` and let\n",
    "        the numerical columns as they are.\n",
    "      * use an `HistGradientBoostingClassifier` as a predictive model.\n",
    "- Make an hyper-parameters search using `RandomizedSearchCV` and tuning the\n",
    "  parameters:\n",
    "      * `learning_rate` with values ranging from 0.001 to 0.5. You can use\n",
    "        an exponential distribution to sample the possible values.\n",
    "      * `l2_regularization` with values ranging from 0 to 0.5. You can use\n",
    "        a uniform distribution.\n",
    "      * `max_leaf_nodes` with values ranging from 5 to 30. The values should\n",
    "        be integer following a uniform distribution.\n",
    "      * `min_samples_leaf` with values ranging from 5 to 30. The values\n",
    "        should be integer following a uniform distribution.\n",
    "\n",
    "In case you have issues of with unknown categories, try to precompute the\n",
    "list of possible categories ahead of time and pass it explicitly to the\n",
    "constructor of the encoder:\n",
    "\n",
    "```python\n",
    "categories = [data[column].unique()\n",
    "              for column in data[categorical_columns]]\n",
    "OrdinalEncoder(categories=categories)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining evaluation and hyper-parameters search\n",
    "\n",
    "Cross-validation was used for searching the best model parameters. We\n",
    "previously evaluate model performance through cross-validation as well. If we\n",
    "would like to combine both aspects, we need to perform a \"nested\"\n",
    "cross-validation. The \"outer\" cross-validation is applied to assess the\n",
    "model while the \"inner\" cross-validation set the hyper-parameters of the\n",
    "model on the data set provided by the \"outer\" cross-validation. In practice,\n",
    "it is equivalent of including, `GridSearchCV`, `RandomSearchCV`, or any\n",
    "`EstimatorCV` in a `cross_val_score` or `cross_validate` function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is: 0.85 +- 0.00\n",
      "The different scores obtained are: \n",
      "[0.8512642  0.84962637 0.84797297 0.85298935 0.85503686]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegressionCV(max_iter=1000, solver='lbfgs', cv=5))\n",
    "score = cross_val_score(model, data, target, n_jobs=4, cv=5)\n",
    "print(\n",
    "    f\"The accuracy score is: {score.mean():.2f} +- {score.std():.2f}\"\n",
    ")\n",
    "print(f\"The different scores obtained are: \\n{score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that such training might involve a variation of the hyper-parameters\n",
    "of the model. When analyzing such model, you should not only look at the\n",
    "overall model performance but look at the hyper-parameters variations as\n",
    "well."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "python_scripts//py:percent,notebooks//ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
